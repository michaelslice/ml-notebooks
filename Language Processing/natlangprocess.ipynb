{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\ops\\distributions\\distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\ops\\distributions\\bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (or NLP for short) is a discipline in computing that deals with the communications between natural (human) languages and computer languages. A common example of NLP is something like spell check or autocomplete. Essentially NLP is the field that focuses on how computers can understand and/or process natural/human languages. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are capable of processing sequential data such as text or characters \n",
    "\n",
    "- Sentiment Analysis\n",
    "- Character Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 3, 8: 1, 9: 4}\n",
      "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "word_encoding = 1\n",
    "def bag_of_words(text):\n",
    "    global word_encoding\n",
    "\n",
    "\n",
    "    words = text.lower().split(\" \")\n",
    "    bag = {}\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            encoding - vocab[word]\n",
    "        else:\n",
    "            vocab[word] = word_encoding\n",
    "            encoding = word_encoding\n",
    "            word_encoding += 1\n",
    "    \n",
    "        if encoding in bag:\n",
    "            bag[encoding] += 1\n",
    "        else:\n",
    "            bag[encoding] = 1\n",
    "    return bag\n",
    "\n",
    "text = \"this is a test to see if this test will work is is test\"\n",
    "bag = bag_of_words(text)\n",
    "print(bag)\n",
    "print(vocab)     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition is the process of computationally identifying and categorizing opinions expressed in a piece of text, especially to determine whether the writer's attitude towards a particular topic, or product is positive, negative, or neutral."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "VOCAB_SIZE = 88584\n",
    "\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words= VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a look at some of our loaded reviews we'll notice that they are different lengths. This is an issue. We cannot pass different lengths of data into our neural network. Therefore we must make each review the same length. To do this we will follow the procedure below.\n",
    "\n",
    "- If the review is greater than 250 words then trim off each the extra words. \n",
    "- If the review is less than 250 words add the necessary amount of 0's to make it equal to 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a word embedding layer as the first layer in our model and add an LSTM layer afterward that feeds into a dense node to get our predicted sentiment.\n",
    "\n",
    "32 stands for the output dimension of the vectors generated by the embedding layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2843041 (10.85 MB)\n",
      "Trainable params: 2843041 (10.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 26s 39ms/step - loss: 0.4623 - acc: 0.7673 - val_loss: 0.3651 - val_acc: 0.8432\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.2659 - acc: 0.8974 - val_loss: 0.3219 - val_acc: 0.8624\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 25s 40ms/step - loss: 0.2034 - acc: 0.9246 - val_loss: 0.3016 - val_acc: 0.8868\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.1627 - acc: 0.9428 - val_loss: 0.4169 - val_acc: 0.8752\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.1345 - acc: 0.9530 - val_loss: 0.3233 - val_acc: 0.8802\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.1114 - acc: 0.9642 - val_loss: 0.3695 - val_acc: 0.8794\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.0974 - acc: 0.9689 - val_loss: 0.3484 - val_acc: 0.8736\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.0818 - acc: 0.9733 - val_loss: 0.3901 - val_acc: 0.8818\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.0687 - acc: 0.9787 - val_loss: 0.4605 - val_acc: 0.8602\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.0607 - acc: 0.9818 - val_loss: 0.4062 - val_acc: 0.8800\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 12ms/step - loss: 0.4792 - acc: 0.8590\n",
      "[0.47918543219566345, 0.858959972858429]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
    "\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thatmoviewasjustamazingsoamazin\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num] + \"\"\n",
    "    return text[:-1]\n",
    "print(decode_integers(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 310ms/step\n",
      "[0.82422864]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[0.41191435]\n"
     ]
    }
   ],
   "source": [
    "# time to make a prediction\n",
    "\n",
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1,250))\n",
    "    pred[0] = encoded_text\n",
    "    result = model.predict(pred)\n",
    "    print(result[0])\n",
    "    \n",
    "positve_review = \"That movie was so awesome! I really loved it and would watch it again because it was amazing\"\n",
    "predict(positve_review)\n",
    "\n",
    "negative_review = \"that movie sucked. I hated it and would not watch it again. Was one of the worst things I've ever watched\" \n",
    "predict(negative_review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Poem/ Play Generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use an RNN to generate a play. We will simply show the RNN an example of something we want it to recreate and it will learn how to write a version of it on its own. We'll do this using a character predictive model that will take as input a variable-length sequence and predict the next character. We can use the model many times in a row with the output from the last predictions as the input for the next call to generate a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os \n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Your own Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# path_to_file = list(files.upload().keys())[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Contents of File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it \n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this text isn't encoded yet well need to do that ourself. We are going to encode each unique character as a different ineger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# lets look at how part of our text is encoded\n",
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", text_to_int(text[:13]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we will make a function that can convert our numeric values to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass \n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering our task is to feed the model a sequence and have it return to us the next character. This means we need to split our text data from above into many shorter sequences that we can pass to the model as training examples.\n",
    "\n",
    "The training we will prepare will use a seq_length sequence as input and a seq_length sequence as the output where that sequence is the original sequence shifted one letter to the right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100 # length of sequence for a training example\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use the batch method to turn this stream of characters into batches of desired length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use these sequences of length 101 and split them into input and output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk): # for the example:hello\n",
    "    input_text = chunk[:-1] # hello\n",
    "    target_text = chunk[1:] # ello\n",
    "    return input_text, target_text # hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target) # we use map to apply the above function to every entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(x))\n",
    "    print(\"\\nOUTPUT\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fianlly we need to make training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab) # vocab is the number of unique characers\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequence,\n",
    "# so it doesn\n",
    "# t attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to build the model. We will use an embedding layer LSTM, and one dense layer that contains a node for each unique character in our training data. The dense layer will give us a probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5330241 (20.33 MB)\n",
      "Trainable params: 5330241 (20.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                         return_sequences=True,\n",
    "                         stateful=True,\n",
    "                         recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create our loss function for this problem. This is because our model will output a (64, sequence_length, 65) shaped tensor that represents the probability distribution of each character at each timestamp for every sequence in the batch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before we do that let's have a look at a sample input and the output from our untrained model. This is so we can understand what the model is actually giving us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch) # ask our model for a prediciton on our first batch of training data\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") # print out the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[ 7.52468314e-03  7.47854589e-04  3.03820800e-03 ...  4.69247065e-03\n",
      "    2.25057499e-03 -1.71929516e-03]\n",
      "  [ 3.83482873e-03  1.91756699e-03 -1.16433250e-03 ...  4.75279661e-03\n",
      "   -6.09286479e-04  1.50424056e-03]\n",
      "  [-2.46454380e-03 -1.70865585e-03  9.38076293e-04 ...  4.78767604e-03\n",
      "   -7.91998580e-03  3.94922495e-03]\n",
      "  ...\n",
      "  [-4.95320652e-04 -2.10995926e-03  6.88036764e-03 ...  4.72197589e-03\n",
      "    1.10435777e-03 -1.68420398e-03]\n",
      "  [ 5.39307063e-03 -4.19754535e-03  3.63111077e-03 ...  5.97755192e-03\n",
      "   -2.14624126e-03 -4.24108002e-03]\n",
      "  [-1.26293593e-03 -7.78077543e-03  6.24626363e-03 ...  5.08348178e-03\n",
      "   -7.26723671e-03 -5.92700962e-04]]\n",
      "\n",
      " [[ 2.14243727e-03 -2.79189786e-04  2.96757440e-03 ...  8.01330898e-05\n",
      "    2.29353923e-03  8.90440890e-04]\n",
      "  [ 2.27318960e-04  1.33099267e-04 -2.00211187e-04 ...  2.57162703e-03\n",
      "    3.06119770e-03  1.68464391e-03]\n",
      "  [-7.50330184e-03  1.02394726e-04  1.75802957e-03 ...  4.57954500e-03\n",
      "    4.85904049e-03  1.19408139e-03]\n",
      "  ...\n",
      "  [-3.42070824e-03 -1.47783188e-02  4.35736589e-03 ...  2.38353969e-03\n",
      "   -1.12504521e-02 -6.74476288e-03]\n",
      "  [-2.37650517e-03 -1.03265932e-02  2.47633969e-03 ...  4.17163782e-03\n",
      "   -4.57253866e-03 -4.14866069e-03]\n",
      "  [ 3.85547639e-04 -3.91518511e-03  8.67614872e-04 ...  4.86119301e-04\n",
      "   -2.10854271e-03 -2.77482392e-03]]\n",
      "\n",
      " [[-9.03989276e-05 -2.61223689e-03 -2.47099320e-04 ... -2.84317182e-03\n",
      "   -6.38977624e-03  6.96098199e-04]\n",
      "  [ 6.27765805e-03 -3.20345350e-03 -2.59493943e-03 ...  9.63102095e-04\n",
      "   -7.22211692e-03 -2.54867692e-03]\n",
      "  [ 3.38054937e-03 -2.61035934e-03 -3.99345532e-03 ...  3.59726930e-03\n",
      "   -2.74830102e-03 -2.47435272e-03]\n",
      "  ...\n",
      "  [-3.44368769e-03 -1.62393600e-03 -3.39983334e-03 ...  1.42611936e-03\n",
      "    2.96803098e-03  5.00492891e-03]\n",
      "  [-1.10977972e-02  1.08303246e-03 -3.70074250e-03 ...  3.00724921e-03\n",
      "    3.18285078e-03  1.55344233e-03]\n",
      "  [-1.39669199e-02  1.13668176e-03 -4.04152041e-03 ...  3.11408984e-03\n",
      "    2.20595626e-03  5.41501446e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 6.25126110e-03 -2.22088955e-03 -3.10325506e-03 ...  2.56144116e-03\n",
      "   -2.11252086e-03 -2.74798390e-03]\n",
      "  [ 3.46010365e-03 -2.47819861e-03 -4.89993487e-03 ...  4.39243857e-03\n",
      "    1.30183797e-03 -2.31904187e-03]\n",
      "  [-3.71164083e-03 -1.36886723e-03 -6.74664183e-03 ...  4.79022227e-03\n",
      "    3.59042082e-04  9.60516336e-04]\n",
      "  ...\n",
      "  [-3.10350722e-03 -3.12661519e-03 -3.13897361e-03 ...  3.13143060e-03\n",
      "    3.17960279e-03  5.37679531e-03]\n",
      "  [-2.85391696e-03 -1.89015223e-03 -2.97232298e-03 ... -3.18952813e-03\n",
      "    2.86150654e-03 -5.58773289e-04]\n",
      "  [-3.50177381e-03  8.01690621e-04 -3.38011654e-04 ... -1.75327912e-03\n",
      "    3.97790968e-03 -3.14314989e-03]]\n",
      "\n",
      " [[-2.15696637e-03 -1.23179972e-03  8.09512450e-04 ... -4.17603087e-03\n",
      "    3.04448698e-03  8.33792705e-03]\n",
      "  [ 3.72018898e-04  2.99913948e-03 -2.80483556e-03 ... -5.95895341e-03\n",
      "    2.20305775e-03  6.31321222e-03]\n",
      "  [ 4.54973662e-03 -7.33653433e-04 -2.76989769e-04 ... -3.89574841e-03\n",
      "    1.47463591e-03  3.94603051e-03]\n",
      "  ...\n",
      "  [ 5.40726352e-03 -7.47414865e-03  1.88577280e-03 ...  6.47949195e-03\n",
      "    1.12620834e-03 -3.55752767e-03]\n",
      "  [ 6.12253835e-03 -6.48760982e-03  6.49983808e-03 ...  5.21835778e-03\n",
      "    4.00366588e-03 -2.09565647e-03]\n",
      "  [-1.24540832e-03 -3.60869104e-03  3.48515343e-03 ...  5.63462079e-03\n",
      "    1.78984180e-03  2.45532580e-03]]\n",
      "\n",
      " [[-3.56855616e-03 -6.92995545e-03  3.84772010e-03 ...  2.16200529e-03\n",
      "   -4.43434203e-03  2.39085616e-03]\n",
      "  [-3.43601406e-03 -4.85502649e-03  7.45058432e-03 ... -3.13510699e-03\n",
      "   -5.72662614e-03  2.16905100e-04]\n",
      "  [-6.54590549e-04 -6.26518484e-03  3.78309051e-03 ... -3.44718480e-03\n",
      "   -2.77641695e-03  1.97084434e-03]\n",
      "  ...\n",
      "  [ 3.87990847e-04 -4.63767536e-03  1.10144159e-02 ...  5.41525427e-04\n",
      "    8.05692188e-03  7.24248821e-04]\n",
      "  [-5.64344041e-03 -4.26487438e-03  1.18207727e-02 ...  3.60297645e-03\n",
      "    8.72323476e-03  5.56518440e-04]\n",
      "  [ 4.80653159e-03 -3.80793563e-03  1.39629785e-02 ...  7.05981487e-03\n",
      "    8.45662691e-03 -1.05762051e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# we can see that the prediction is an array of 64 array, one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[ 0.00752468  0.00074785  0.00303821 ...  0.00469247  0.00225057\n",
      "  -0.0017193 ]\n",
      " [ 0.00383483  0.00191757 -0.00116433 ...  0.0047528  -0.00060929\n",
      "   0.00150424]\n",
      " [-0.00246454 -0.00170866  0.00093808 ...  0.00478768 -0.00791999\n",
      "   0.00394922]\n",
      " ...\n",
      " [-0.00049532 -0.00210996  0.00688037 ...  0.00472198  0.00110436\n",
      "  -0.0016842 ]\n",
      " [ 0.00539307 -0.00419755  0.00363111 ...  0.00597755 -0.00214624\n",
      "  -0.00424108]\n",
      " [-0.00126294 -0.00778078  0.00624626 ...  0.00508348 -0.00726724\n",
      "  -0.0005927 ]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#  lets examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)\n",
    "# notice this is a 2d array of length 100, where each interior is the prediction for the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[ 7.5246831e-03  7.4785459e-04  3.0382080e-03 -3.4172705e-03\n",
      " -5.7150521e-03 -2.4804785e-03 -5.2539934e-03  5.0792156e-04\n",
      " -8.5141417e-04 -1.1780384e-04 -6.6540819e-03  4.1186796e-03\n",
      "  8.3103720e-03  2.1683308e-03 -3.5964467e-03  4.3471921e-03\n",
      "  3.5418123e-03 -7.9033207e-03  1.8901996e-03  4.2487960e-03\n",
      " -8.4591418e-04 -1.8868948e-03 -1.0837242e-03 -2.7089042e-03\n",
      " -5.0206273e-03 -1.2774453e-03  8.9711370e-04 -9.2898821e-03\n",
      " -1.0478126e-03 -1.4666410e-03 -3.4905844e-03  4.8223003e-03\n",
      "  1.5959326e-03 -1.9189032e-03  2.1222052e-03 -5.9363537e-04\n",
      " -5.4394943e-05  5.3489273e-03  4.7227899e-03 -1.8341374e-03\n",
      " -7.5931605e-03 -2.0364928e-03  7.1149152e-03 -9.2580519e-04\n",
      " -1.1237874e-04 -1.0232350e-03 -1.7354682e-03 -1.2888259e-03\n",
      "  5.9911227e-03 -5.4257149e-03  5.7432945e-03  2.9295245e-03\n",
      " -2.1900458e-05  1.7815575e-03 -8.8213728e-04  6.0356851e-03\n",
      " -3.2999432e-03 -4.5606797e-03  4.4665062e-03 -1.2756911e-03\n",
      "  2.8614053e-03  6.8711741e-03  4.6924707e-03  2.2505750e-03\n",
      " -1.7192952e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# and finally well look ata prediction at the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "# and of course its 65 values representing the probabillity of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A,g,pGhVyNijQ?RHYT.epfZPxku3xAeGf'R3fBLfNyQTn?'PD,-Fnipet,e\\noA.HgoJ3mzlefKDCajo,$.oLOwEb?atB\\nAKo\\nHln\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabilites)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# now we can reshpae that array and ocnvert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars # and this is what the model preidcted for training sequence 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we need to create our own loss function thaht can compare that output to the expected output and give us some numeric value representing how close the two were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can think of our problem as a classification problem where the model predicts the probability of each unique letter coming next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Checkpoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to setup and configure our model to save checkpoints as it trains. This will allow us to load our model from a checkpoint and continue training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durectiry where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 510s 3s/step - loss: 2.6610\n",
      "Epoch 2/5\n",
      "172/172 [==============================] - 557s 3s/step - loss: 1.9556\n",
      "Epoch 3/5\n",
      "172/172 [==============================] - 558s 3s/step - loss: 1.6948\n",
      "Epoch 4/5\n",
      "172/172 [==============================] - 569s 3s/step - loss: 1.5512\n",
      "Epoch 5/5\n",
      "172/172 [==============================] - 571s 3s/step - loss: 1.4648\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=5, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll rebuild the model from a checkpoint using a batch_size of 1 so that we can feed one piece of text to the model and have it make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is finished training we can find the lastest checkpoint that stores the models weights using the follow lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load any checkpoint we want by specifying the exact file to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_num = 10\n",
    "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation steps (generating text using the learned model)\n",
    "    \n",
    "    # Number of characters to generate \n",
    "    num_generate = 800\n",
    "    \n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    # Empty string to store our results \n",
    "    text_generated = []\n",
    "    \n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in moer suprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    \n",
    "    temperature = 1.0\n",
    "    \n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range (num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0 )\n",
    "        \n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        # we pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gangue she had,\n",
      "The nabling naturl: light in caniears\n",
      "That if is lipht's kindless headerst hamph\n",
      "And save the curmen shord unto the sturn and men.\n",
      "\n",
      "WORWER:\n",
      "The griWedest that you say it is.\n",
      "\n",
      "LEONTES:\n",
      "We should\n",
      "gentle a thousands; my die eye wears\n",
      "What I may need newled it, to do you;\n",
      "If I absuge, and in myself consperitring\n",
      "The deforus shall op's hourd after me,\n",
      "And I have put wear so great thy much,\n",
      "Encend five; but seem swe withal son cry accose\n",
      "Pusit but the crows. Whose presently?\n",
      "\n",
      "First Wellon:\n",
      "What he is not thy heaven, death?\n",
      "\n",
      "ROMEO:\n",
      "O bear, granden some vows bet thee, comestur: though are galle's barigain\n",
      "In show me did all mase.\n",
      "\n",
      "CLARENCE:\n",
      "With a villain!\n",
      "Canst, I am some fime axaling-all break.\n",
      "\n",
      "ROMEO:\n",
      "Out formest than and false bearing;\n",
      "To ston in requifuse night not come here,\n",
      "bow \n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
