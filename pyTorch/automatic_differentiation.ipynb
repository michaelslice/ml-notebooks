{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x0000024C73D67D90>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x0000024C73D67910>\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "\n",
    "# When training neural networks the most frequently used algorithm is back propagation, in this algorithm\n",
    "# models weights are adjusted according to the gradient of the loss function with respect to the given parameter \n",
    "\n",
    "# To compute gradients we can use `torch.autograd`\n",
    "\n",
    "import torch \n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "# In this network w and b are parameters which we need to optimize. Thus we need to be able to compute \n",
    "# the gradients of loss function with respect to those variables, so we set `required_grad` property of those tensors\n",
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0785, 0.0115, 0.0034],\n",
      "        [0.0785, 0.0115, 0.0034],\n",
      "        [0.0785, 0.0115, 0.0034],\n",
      "        [0.0785, 0.0115, 0.0034],\n",
      "        [0.0785, 0.0115, 0.0034]])\n",
      "tensor([0.0785, 0.0115, 0.0034])\n"
     ]
    }
   ],
   "source": [
    "# To optimize weights in the neural network, we need to compute the derivative of our loss function with respect to its parameters\n",
    "# To compute those derivatives we call `loss.backward()`\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disabling Gradient Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# By default all tensors require `requires_grad=True` are tracking their computational history and support gradient computation\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "# There are some reasons why you might want to disable gradient tracking\n",
    "# - To mark some parameters in your neural network as frozen parameters\n",
    "# - To speed up computations when you are only doing a forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually autograd keeps a record of data(tensors) and all executed operations (along with the resulting new tensors) in a \n",
    "directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input sensors, roots are the output tensors.\n",
    "By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
